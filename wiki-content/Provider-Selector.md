
<!-- SYNCED_WIKI_PAGE: Do not edit in the GitHub Wiki UI. This page is synced from wiki-content/ in the repository. -->
> Source of truth lives in README.md and docs/. Make changes via PRs to the repo; CI auto-publishes to the Wiki.

# Provider Selector

> Use this quick decision tree to choose a starting provider. Each outcome links to the detailed setup guides.

```text
Start
 â”œâ”€â”€ Need to stay 100% offline or avoid API keys?
 â”‚     â”œâ”€â”€ Yes â†’ [Ollama](Local-Providers#ðŸ¦™-ollama-most-popular-local)
 â”‚     â””â”€â”€ Prefer a GUI for local models â†’ [LM Studio](Local-Providers#ðŸŽ¬-lm-studio-gui-based-local)
 â”œâ”€â”€ Looking for the lowest cloud cost?
 â”‚     â””â”€â”€ [DeepSeek](Cloud-Providers#ðŸ§ -deepseek-ultra-low-cost-leader) (add Gemini Flash as fallback)
 â”œâ”€â”€ Need generous free tier and speed?
 â”‚     â””â”€â”€ [Gemini Flash](Cloud-Providers#âœ¨-google-gemini-speed--free-tier-champion)
 â”œâ”€â”€ Want access to many premium models with one key?
 â”‚     â””â”€â”€ [OpenRouter](Cloud-Providers#openrouter)
 â”œâ”€â”€ Highest-quality reasoning (budget OK)?
 â”‚     â””â”€â”€ [Anthropic Claude 3.5](Cloud-Providers#anthropic)
 â””â”€â”€ Fastest responses possible?
       â””â”€â”€ [Groq Llama 3.x](Cloud-Providers#ðŸš€-groq-lightning-fast-inference)
```

## Recommended fallback chains

| Goal | Primary | Fallback(s) |
|------|---------|-------------|
| Stay offline | Ollama | LM Studio |
| Budget cloud | DeepSeek | Gemini Flash â†’ OpenRouter (GPT-4.1 Mini) |
| Premium quality | Claude 3.5 Sonnet (via OpenRouter) | GPT-4o â†’ DeepSeek Chat |
| Speed | Groq Llama 3.2 | OpenRouter (Gemini Flash) â†’ Local Ollama |

After choosing a starting point, follow the [Provider Setup Hub](Provider-Setup.md) and then run through the [Operations Playbook](Operations.md) to validate.
